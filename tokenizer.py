"""
Text tokenizer.

Functions to extract tokens from string sequences.

"""

import re
import unicodedata


def unicode(str):
    """Normalize unicode data to remove accents, etc."""
    return unicodedata.normalize(
        'NFKD', str).encode('ASCII', 'ignore').decode('utf-8')


def split_sequence(sequence, pattern):
    """
    Splits a sequence of words into a list of tokens.

    Parameters:
        sequence (str): string containing tokens.
            Ex.: 'eva maria de los angeles rodriguez lopez'
        pattern (str): regular expression used to identify tokens.

    Returns:
        tokens (list): list of tokens.
            Ex.: ['eva', 'maria', 'de los', 'angeles', 'rodriguez', 'lopez']
    """
    return re.findall(pattern, sequence)


def subtokens(tokens, length, particles):
    """
    Create a list of subtokens from a list of tokens.

    Takes the last 'lenght' characters of each token, or the whole particle.

    Parameters:
        tokens (list): list of tokens as generated by method split_sequence.
            Ex.: ['eva', 'maria', 'de los', 'angeles', 'rodriguez', 'lopez']
        length (int): length of generated subtokens. Ex.: 4
            If len(token) <= length then subtoken = token
        particles (list): list of special tokens not to be transformed.

    Returns:
        subtokens (list): list of subtokens with the specified length.
            Ex.: ['eva', 'aria', 'de los', 'eles', 'guez', 'opez']
    """
    subtokens = []
    for token in tokens:
        if token not in particles:
            subtoken = token[-length:]
        else:
            subtoken = token
        subtokens.append(subtoken)
    return subtokens
